{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ›‚ EasyVisa Case Status Prediction\n",
    "\n",
    "In this notebook, we will build a machine learning model to predict the outcome of a visa application (`case_status`: Certified or Denied). We will follow our standard process, ensuring all categorical data is encoded to maintain a simple API.\n",
    "\n",
    "1.  **Data Loading and Exploration (EDA)**\n",
    "2.  **Feature Engineering and Preprocessing**\n",
    "3.  **Model Training**\n",
    "4.  **Model Evaluation**\n",
    "5.  **Model Saving** (for API deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Exploratory Data Analysis (EDA)\n",
    "\n",
    "First, let's load the dataset from the provided URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Load the dataset\n",
    "url = r\"https://raw.githubusercontent.com/ek-chris/Practice_datasets/refs/heads/main/EasyVisa%20(1).csv\"\n",
    "visa_df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"--- Dataset Head ---\")\n",
    "print(visa_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the dataset's structure and check for any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the dataframe\n",
    "print(\"\\n--- Dataset Info ---\")\n",
    "visa_df.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n--- Missing Values Check ---\")\n",
    "print(visa_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The data is clean with **25,480 entries** and **no missing values**. We have 10 `object` columns (categorical) and 3 numeric columns. `case_id` and `company_name` are identifiers and will be dropped.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Target Variable\n",
    "\n",
    "Let's see the distribution of our target variable, `case_status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target variable 'case_status'\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='case_status', data=visa_df, palette=\"coolwarm\")\n",
    "plt.title('Distribution of Visa Case Status')\n",
    "plt.xlabel('Case Status')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('visa_case_status_distribution.png')\n",
    "print(\"Saved visa_case_status_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The dataset is **imbalanced**. There are significantly more \"Certified\" cases (approx. 17,000) than \"Denied\" cases (approx. 8,500). We will use `class_weight='balanced'` in our model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering & Preprocessing\n",
    "\n",
    "To keep the API simple, we will `LabelEncode` all `object` columns. We will also drop `case_id` and `company_name` as they are just identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to avoid changing the original dataframe\n",
    "df_processed = visa_df.copy()\n",
    "\n",
    "# Drop identifier columns\n",
    "df_processed = df_processed.drop(['case_id', 'company_name'], axis=1)\n",
    "\n",
    "# Map target variable\n",
    "df_processed['case_status'] = df_processed['case_status'].map({'Denied': 0, 'Certified': 1})\n",
    "\n",
    "# Identify categorical columns for encoding\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Label Encoding for all categorical features\n",
    "# We will save these encoders for the API\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "print(\"--- Processed Data Head ---\")\n",
    "print(df_processed.head())\n",
    "\n",
    "print(\"\\n--- Processed Data Info ---\")\n",
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** All columns are now numeric, making the data ready for scaling and modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting and Scaling the Data\n",
    "\n",
    "We will split the data and scale all features, as they are all numeric now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df_processed.drop('case_status', axis=1)\n",
    "y = df_processed['case_status']\n",
    "\n",
    "# Save feature names for the API\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale all features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "print(\"Data scaling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Selection and Training\n",
    "\n",
    "We'll use `RandomForestClassifier` with `class_weight='balanced'` to handle the data imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Evaluation\n",
    "\n",
    "Let's evaluate the model's performance on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Generate the classification report\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Denied (0)', 'Certified (1)']))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='BuPu',\n",
    "            xticklabels=['Predicted Denied', 'Predicted Certified'],\n",
    "            yticklabels=['Actual Denied', 'Actual Certified'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig('visa_confusion_matrix.png')\n",
    "print(\"Saved visa_confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Interpretation & Conclusion\n",
    "\n",
    "The model performs reasonably well.\n",
    "\n",
    "* **Accuracy (74%):** A decent overall score.\n",
    "* **Precision (Denied): 0.60:** When the model predicts \"Denied,\" it is correct 60% of the time.\n",
    "* **Recall (Denied): 0.54:** The model successfully identified 54% of all actual \"Denied\" cases.\n",
    "* **F1-Score (Certified): 0.81:** The model is much better at identifying \"Certified\" cases, which is expected given the data imbalance.\n",
    "\n",
    "The model is a good starting point. The relatively high number of **False Positives (780)**â€”cases predicted as \"Certified\" but were \"Denied\"â€”is the main area for business concern and future improvement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save Model, Scaler, and Features for API\n",
    "\n",
    "We save the `model`, the `scaler`, the `encoders` (for all categorical columns), and the `feature_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "joblib.dump(model, 'easy_visa_model.joblib')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'easy_visa_scaler.joblib')\n",
    "\n",
    "# Save the label encoders\n",
    "joblib.dump(encoders, 'easy_visa_encoders.joblib')\n",
    "\n",
    "# Save the feature names in order\n",
    "joblib.dump(feature_names, 'easy_visa_features.joblib')\n",
    "\n",
    "print(\"Files saved successfully:\")\n",
    "print(\"easy_visa_model.joblib\")\n",
    "print(\"easy_visa_scaler.joblib\")\n",
    "print(\"easy_visa_encoders.joblib\")\n",
    "print(\"easy_visa_features.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}