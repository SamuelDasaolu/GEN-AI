{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, accuracy_score, confusion_matrix\n",
    "\n",
    "# Set a style for our plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# set pandas parameters\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ],
   "id": "ade95fff3e539b24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "Phase 1: Data Collection and Preparation\n",
    "Task 1.1: Load the dataset from here.\n",
    "Task 1.2: Load the dataset into a Pandas DataFrame.\n",
    "Task 1.3: Inspect the dataset for missing values and handle them appropriately.\n",
    "Task 1.4: Perform data cleaning to ensure the dataset is ready for analysis.\n"
   ],
   "id": "d8dd27aaeab250ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_data_from_url(url: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Attempts to load a CSV from a URL into a DataFrame.\n",
    "    Returns the DataFrame on success or None on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = pd.read_csv(url)\n",
    "        if response is not None:\n",
    "            print(\"--- Data Loaded Successfully ---\")\n",
    "            print(response.head())\n",
    "            return response\n",
    "        else:\n",
    "            print(\"Data loading failed.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "DATA_URL = r\"https://raw.githubusercontent.com/ek-chris/Practice_datasets/refs/heads/main/EasyVisa%20(1).csv\"\n",
    "\n",
    "data = load_data_from_url(DATA_URL)\n",
    "\n"
   ],
   "id": "7c0cd25276fe8363"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_missing_value_counts(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Calculates the count of nulls for each column.\"\"\"\n",
    "    return df.isnull().sum()\n",
    "\n",
    "\n",
    "def get_duplicate_row_count(df: pd.DataFrame) -> int:\n",
    "    \"\"\"Calculates the total number of duplicate rows.\"\"\"\n",
    "    return df.duplicated().sum()\n",
    "\n",
    "\n",
    "missing_counts = get_missing_value_counts(data)\n",
    "duplicate_count = get_duplicate_row_count(data)\n",
    "print(\"\\n--- Missing Value Counts ---\")\n",
    "print(missing_counts)\n",
    "\n",
    "print(f\"\\n--- Duplicate Rows ---\")\n",
    "print(duplicate_count)"
   ],
   "id": "d3f572d6e1b88984"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "# Phase 2: Exploratory Data Analysis (EDA)\n",
    "Task 2.1: Conduct exploratory data analysis to understand the distribution of features and the target variable .\n",
    "Task 2.2: Visualize the relationships between features and the target variable using scatter plots, histograms, and box plots.\n",
    "Task 2.3: Identify and handle outliers in the dataset.\n"
   ],
   "id": "b251b3b168dc69c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_statistical_description(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Returns the .describe() output for numerical columns.\"\"\"\n",
    "    return df.describe()\n",
    "\n",
    "\n",
    "def get_skewness(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Returns the skewness for numerical columns.\"\"\"\n",
    "    return df.skew(numeric_only=True)\n",
    "\n",
    "\n",
    "print(\"--- Statistical Description of Numerical Data ---\")\n",
    "print(get_statistical_description(data))\n",
    "\n",
    "print(\"\\n--- Skewness ---\")\n",
    "print(get_skewness(data))"
   ],
   "id": "1536621c41a38e1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# visualize the distribution between the features and target varable\n",
    "# Analyze the target variable 'case_status'\n",
    "print(\"\\n--- Target Variable Distribution (case_status) ---\")\n",
    "print(data['case_status'].value_counts())\n",
    "print(\"\\n--- Target Variable Proportions (case_status) ---\")\n",
    "print(data['case_status'].value_counts(normalize=True))"
   ],
   "id": "a024666a58616cbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Insight:\n",
    "```\n",
    "The dataset is imbalanced. About 66.78 percent of the cases were approved(\"Certified\") while   33.21% were rejected (\"Denied\"). This is important to remeber for modelling later"
   ],
   "id": "9b0a58668307c610"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize the relationships between features and the target variable using scatter plots, histograms, and box plots.",
   "id": "97ed057caa21deb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_histogram(ax: plt.Axes, data: pd.Series, bins: int, title: str):\n",
    "    \"\"\"Takes a matplotlib axis and plots a histogram on it.\"\"\"\n",
    "    sns.histplot(data, kde=True, bins=bins, ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_boxplot(ax: plt.Axes, data: pd.Series, title: str):\n",
    "    \"\"\"Takes a matplotlib axis and plots a boxplot on it.\"\"\"\n",
    "    sns.boxplot(x=data, ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "numerical_features = ['no_of_employees', 'yr_of_estab', 'prevailing_wage']\n",
    "plot_bins = [50, 40, 40]  # Bins for each feature\n",
    "\n",
    "print(\"---Visual Distributions of Numerical Features---\")\n",
    "# Set up a figure for 3 rows and 2 columns\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 15))\n",
    "\n",
    "# We'll loop through our features and plot them\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    # Get the specific data series\n",
    "    data_series = data[feature]\n",
    "\n",
    "    # Get the correct axes for this row\n",
    "    hist_ax = axes[i, 0]\n",
    "    box_ax = axes[i, 1]\n",
    "\n",
    "    plot_histogram(hist_ax, data_series,\n",
    "                   bins=plot_bins[i],\n",
    "                   title=f'{feature} Distribution (Skewed)')\n",
    "\n",
    "    plot_boxplot(box_ax, data_series,\n",
    "                 title=f'{feature} Box Plot (Shows Outliers)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f11b10bf6fabd721"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Insights\n",
    "\n",
    "    The visualizations confirm our findings from describe(). All three features are heavily skewed and have significant outliers.\n",
    "\n",
    "    Prevailing Wage is interesting: a large number of applicants have 0 prevailing wage, creating a large spike at 0.\n",
    "\n",
    "    This skewness suggests that for modeling, we might need to apply a log transformation to these features to normalize their distributions."
   ],
   "id": "d5088a8f7e79ed90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_countplot_on_ax(ax: plt.Axes, data: pd.DataFrame, col: str):\n",
    "    \"\"\"\n",
    "    Takes a matplotlib axis, plots a countplot for a specific column, and sets the title.\n",
    "    \"\"\"\n",
    "    sns.countplot(x=col, data=data, ax=ax)\n",
    "    ax.set_title(f'Distribution of {col}')\n",
    "\n",
    "\n",
    "categorical_features = ['continent', 'education_of_employee', 'has_job_experience', 'requires_job_training',\n",
    "                        'region_of_employment', 'unit_of_wage', 'full_time_position']\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 22))\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_features):\n",
    "    plot_countplot_on_ax(ax=axes_flat[i], data=data, col=col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6b3a9d14dd45096b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Insights:\n",
    "    continent: Majority of the Applicants are from Asia, then Europe, North America, South America, Africa and Oceania in that order\n",
    "\n",
    "    education_of_employee: Majority of Applicants Have Either A Bachelor's or A Master's Degree, with fewer having just the High School Diploma or Doctorate\n",
    "\n",
    "    has_job_experience: Job Experience is Split Fairly Evenly but with more apllicants having prior job experience\n",
    "\n",
    "    requires_job_training: Majority of Applicants dont require Job Training\n",
    "\n",
    "    region_of_employment:\n",
    "\n",
    "    unit_of_wage: Most Applicants are to be paid per year\n",
    "\n",
    "    full_time_position: Most Employers are offering full time positions\n"
   ],
   "id": "4fde17a618d1c3d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Categorical Features Against Case Status (Target Variable)",
   "id": "6ee32182e1400f7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_countplot_with_hue(ax: plt.Axes, data: pd.DataFrame, x_col: str, hue_col: str):\n",
    "    \"\"\"\n",
    "    Takes a matplotlib axis, plots a countplot for a specific column, and breaks it down by a hue.\n",
    "    \"\"\"\n",
    "    sns.countplot(x=x_col, hue=hue_col, data=data, ax=ax)\n",
    "    ax.set_title(f'Case Status by {x_col}')\n",
    "\n",
    "\n",
    "# Set up the plot grid\n",
    "fig, axes = plt.subplots(4, 2, figsize=(18, 24))\n",
    "axes_flat = axes.flatten()\n",
    "for i, col in enumerate(categorical_features):\n",
    "    plot_countplot_with_hue(\n",
    "        ax=axes_flat[i],\n",
    "        data=data,\n",
    "        x_col=col,\n",
    "        hue_col='case_status'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "b4333b074632e4e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Insights",
   "id": "6c9107ba3f37585e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Numerical Features vs Case Status",
   "id": "959e73d38c80c75d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_boxplot_on_ax(ax: plt.Axes, data: pd.DataFrame, x_col: str, y_col: str):\n",
    "    \"\"\"\n",
    "    Takes a matplotlib axis and plots a boxplot for\n",
    "    a given x and y column.\n",
    "    \"\"\"\n",
    "    sns.boxplot(x=x_col, y=y_col, data=data, ax=ax)\n",
    "    ax.set_title(f'{y_col} by {x_col}')\n",
    "\n",
    "\n",
    "# Set up a figure for 3 plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 7))\n",
    "\n",
    "# Loop through our features and the axes at the same time\n",
    "for ax, y_feature in zip(axes, numerical_features):\n",
    "    plot_boxplot_on_ax(\n",
    "        ax=ax,\n",
    "        data=data,\n",
    "        x_col='case_status',\n",
    "        y_col=y_feature\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "fb6445350f760787"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Insight\n",
    "\n",
    "For all three continuous features, the median and interquartile range (the \"box\") are very similar for both 'Y' and 'N' groups.\n",
    "\n",
    "This suggests that there is no simple, linear relationship (e.g., \"higher prevailing wage = approval\"). The relationship is more complex. The outliers are present in both groups.\n"
   ],
   "id": "77c002c560a6df6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Scatter Plots",
   "id": "dab30651c856fc76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_log_transformed_df(df: pd.DataFrame, cols_to_transform: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a new DataFrame with log-transformed (log1p) columns.\n",
    "    Returns a copy, leaving the original DataFrame unchanged.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in cols_to_transform:\n",
    "        df_copy[f'{col}_log'] = np.log1p(df_copy[col])\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def plot_scatterplot_with_hue(df: pd.DataFrame, x_col: str, y_col: str, hue_col: str):\n",
    "    \"\"\"\n",
    "    Generates a styled scatter plot with a hue.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=x_col, y=y_col, hue=hue_col, data=df, alpha=0.7)\n",
    "    plt.title(f'{y_col} vs. {x_col} by {hue_col}')\n",
    "    plt.xlabel(f'{x_col} (Log-Transformed)')\n",
    "    plt.ylabel(f'{y_col} (Log-Transformed)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_df = create_log_transformed_df(df=data, cols_to_transform=['prevailing_wage', 'no_of_employees'])\n",
    "\n",
    "plot_scatterplot_with_hue(df=plot_df, x_col='prevailing_wage_log', y_col='no_of_employees_log', hue_col='case_status')"
   ],
   "id": "c8d4a9b9f22af776"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Insights:\n",
    "```\n",
    "As the prevailing wage increases past a certain point, the frequency of approved applications increase regardless of the no_of_employees\n",
    "\n",
    "This indicates that prevailing wage is a very strong factor"
   ],
   "id": "dad98860b03f8f06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Phase 3: Data Preprocessing\n",
    "```\n",
    "Task 3.1: Create new features that may help improve the model's performance.\n",
    "Task 3.2: Encode categorical variables using one-hot or label encoding techniques.\n",
    "Task 3.3: Normalize or standardize numerical features as needed.\n"
   ],
   "id": "3c4b06cc506cf98f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "standardize all wages to a single, annual unit ('Year'). create a new feature called annual_wage.\n",
    "\n",
    "    If unit_of_wage is 'Year', annual_wage = prevailing_wage\n",
    "\n",
    "    If unit_of_wage is 'Month', annual_wage = prevailing_wage * 12\n",
    "\n",
    "    If unit_of_wage is 'Week', annual_wage = prevailing_wage * 52\n",
    "\n",
    "    If unit_of_wage is 'Hour', annual_wage = prevailing_wage * 40 (hours/week) * 52 (weeks/year)\n",
    "\n",
    "This will make the prevailing_wage (and the new annual_wage) feature much more useful and will solve the outlier problem at the low end."
   ],
   "id": "95c7805215ff4e7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_annual_wage(row) -> float:\n",
    "    \"\"\"Converts a wage to an annual salary based on its unit.\"\"\"\n",
    "    wage = row['prevailing_wage']\n",
    "    unit = row['unit_of_wage']\n",
    "\n",
    "    if unit == 'Year':\n",
    "        return wage\n",
    "    elif unit == 'Month':\n",
    "        return wage * 12\n",
    "    elif unit == 'Week':\n",
    "        return wage * 52\n",
    "    elif unit == 'Hour':\n",
    "        # Assuming a 40-hour work week\n",
    "        return wage * 40 * 52\n",
    "    return wage\n",
    "\n",
    "\n",
    "def create_engineered_features(df: pd.DataFrame, reference_year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes the raw DataFrame and returns a new one with 'annual_wage'\n",
    "    and 'company_age' features, dropping the original columns.\n",
    "    Drops Case ID: Unnecessary Identifier\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    df_new['annual_wage'] = df_new.apply(calculate_annual_wage, axis=1)\n",
    "    df_new['company_age'] = reference_year - df_new['yr_of_estab']\n",
    "    cols_to_drop = [\n",
    "        'prevailing_wage', 'unit_of_wage',  # Replaced by annual_wage\n",
    "        'yr_of_estab',  # Replaced by company_age\n",
    "        'case_id'  # Irrelevant identifier\n",
    "    ]\n",
    "    df_new = df_new.drop(columns=cols_to_drop)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "def plot_annual_wage_boxplot(df: pd.DataFrame):\n",
    "    \"\"\"Generates a styled boxplot for the annual wage vs. case status.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Using 'x' for the categorical 'case_status' is more standard\n",
    "    sns.boxplot(x='case_status', y='annual_wage', data=df, palette='coolwarm')\n",
    "    plt.title('Standardized Annual Wage vs. Case Status')\n",
    "    plt.ylabel('Annual Wage (Standardized)')\n",
    "    plt.xlabel('Case Status')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "REFERENCE_YEAR = 2016\n",
    "print(\"Creating engineered features ('annual_wage', 'company_age')...\")\n",
    "df_processed = create_engineered_features(data, reference_year=REFERENCE_YEAR)\n",
    "print(\"Original columns dropped.\")\n",
    "\n",
    "# Describe new 'annual_wage' distribution\n",
    "print(\"\\n--- New 'annual_wage' feature ---\")\n",
    "print(df_processed['annual_wage'].describe())\n",
    "\n",
    "# visualize it\n",
    "plot_annual_wage_boxplot(df_processed)"
   ],
   "id": "b0bd0ae2ef0d01d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Encode categorical variables\n",
    "\n",
    "## standardize numerical features\n",
    "\n"
   ],
   "id": "7447b9934148d58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def encode_dataframe(df: pd.DataFrame, binary_map: Dict[str, int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a DataFrame and applies all necessary encodings for modeling.\n",
    "    Returns a new, fully encoded DataFrame.\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    # 1. Encode the target variable (Explicitly)\n",
    "    print(\"Mapping target variable 'case_status'...\")\n",
    "    target_map = {'Certified': 1, 'Denied': 0}\n",
    "    df_encoded['case_status'] = df_encoded['case_status'].map(target_map)\n",
    "    print(\"Mapping complete: Certified=1, Denied=0\")\n",
    "\n",
    "    # 2. Encode binary ('Y'/'N') features using the provided map\n",
    "    binary_cols = [col for col in binary_map.keys() if col in df_encoded.columns]\n",
    "    for col in binary_cols:\n",
    "        df_encoded[col] = df_encoded[col].map(binary_map[col])\n",
    "\n",
    "    # 3. One-hot encode multi-category features\n",
    "    multi_cat_cols = ['continent', 'education_of_employee', 'region_of_employment']\n",
    "    df_encoded = pd.get_dummies(df_encoded, columns=multi_cat_cols, drop_first=True)\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "def split_data(df: pd.DataFrame, target_col: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"Splits the DataFrame into features (X) and target (y), then into training and testing sets.\"\"\"\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "def scale_features(\n",
    "        X_train: pd.DataFrame, X_test: pd.DataFrame, numerical_cols: List[str]\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Fits a StandardScaler on the training data and transforms both training and testing data.\n",
    "    Returns new scaled DataFrames and the fitted scaler object.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Create copies to avoid the SettingWithCopyWarning\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "\n",
    "    # Fit on training data and transform both sets\n",
    "    X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "    X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "\n",
    "# Define the mapping for binary features to make the function more generic\n",
    "BINARY_FEATURE_MAP = {\n",
    "    'has_job_experience': {'Y': 1, 'N': 0},\n",
    "    'requires_job_training': {'Y': 1, 'N': 0},\n",
    "    'full_time_position': {'Y': 1, 'N': 0}\n",
    "}\n",
    "NUMERICAL_FEATURES = ['no_of_employees', 'company_age', 'annual_wage']\n",
    "\n",
    "# 1. Encode the entire dataset first\n",
    "print(\"Encoding DataFrame...\")\n",
    "df_final = encode_dataframe(df_processed, binary_map=BINARY_FEATURE_MAP)\n",
    "print(\"Data after encoding:\")\n",
    "print(df_final.head())\n",
    "print(f\"\\nTotal number of features after encoding: {len(df_final.columns) - 1}\")\n",
    "\n",
    "# 2. Split the encoded data into training and testing sets\n",
    "print(\"\\nSplitting data into training and testing sets (80/20 split)...\")\n",
    "X_train, X_test, y_train, y_test = split_data(df_final, target_col='case_status')\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# 3. Scale the numerical features using the train/test split\n",
    "print(\"\\nStandardizing numerical features...\")\n",
    "X_train_scaled, X_test_scaled, fitted_scaler = scale_features(X_train, X_test, NUMERICAL_FEATURES)\n",
    "print(\"Numerical features standardized.\")\n",
    "print(\"\\nFirst 5 rows of X_train (scaled):\")\n",
    "print(X_train_scaled.head())"
   ],
   "id": "7b63d9756a318c6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_and_evaluate_model(\n",
    "        model: ClassifierMixin,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.Series,\n",
    "        X_test: pd.DataFrame,\n",
    "        y_test: pd.Series\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Trains a given model and evaluates its performance on the test set.\n",
    "    Returns a dictionary containing performance metrics and prediction results.\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive AUC ROC class\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Package the results into a dictionary\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"AUC-ROC\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"Precision (Certified)\": report['1']['precision'],\n",
    "        \"Recall (Certified)\": report['1']['recall'],\n",
    "        \"F1-Score (Certified)\": report['1']['f1-score'],\n",
    "        \"fpr\": roc_curve(y_test, y_pred_proba)[0],  # False Positive Rate for ROC curve\n",
    "        \"tpr\": roc_curve(y_test, y_pred_proba)[1],  # True Positive Rate for ROC curve\n",
    "        \"report_str\": classification_report(y_test, y_pred, target_names=['Denied (0)', 'Certified (1)']),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(ax: plt.Axes, conf_matrix: np.ndarray, title: str):\n",
    "    \"\"\"Takes a matplotlib axis and plots a confusion matrix on it.\"\"\"\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xticklabels(['Denied', 'Certified'])\n",
    "    ax.set_yticklabels(['Denied', 'Certified'])\n",
    "\n",
    "\n",
    "# Initialize the models to be tested\n",
    "models_to_run = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# This will hold the results from each model run\n",
    "all_results = {}\n",
    "\n",
    "print(\"--- Training and Evaluating Models ---\")\n",
    "\n",
    "# Setup plots for confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "# Loop, call our function, and store the results\n",
    "for i, (name, model) in enumerate(models_to_run.items()):\n",
    "    print(f\"\\n--------- Processing: {name} ------------\")\n",
    "\n",
    "    # Note: We are now using the scaled data from the previous step\n",
    "    results = train_and_evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    all_results[name] = results\n",
    "\n",
    "    # Print the text-based report\n",
    "    print(f\"Accuracy: {results['Accuracy']:.4f}\")\n",
    "    print(f\"AUC-ROC: {results['AUC-ROC']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(results['report_str'])\n",
    "\n",
    "    # Generate the confusion matrix plot for this model\n",
    "    plot_confusion_matrix(axes_flat[i], results['confusion_matrix'], title=name)\n",
    "    print(f\"\\n--------- Done With Processing: {name} ------------\")\n",
    "\n",
    "# Display all confusion matrices together\n",
    "fig.suptitle(\"Model Confusion Matrices\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# Now, create a single plot for all ROC curves to compare them directly\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, results in all_results.items():\n",
    "    plt.plot(results['fpr'], results['tpr'], label=f\"{name} (AUC = {results['AUC-ROC']:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')  # Dashed line for reference\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison for All Models')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Finally, convert results to a DataFrame for easy comparison\n",
    "# We exclude the raw plot data (fpr, tpr, etc.) from the summary table\n",
    "summary_results = {name: {k: v for k, v in res.items() if isinstance(v, (int, float))}\n",
    "                   for name, res in all_results.items()}\n",
    "results_df = pd.DataFrame(summary_results).T.sort_values(by=\"F1-Score (Certified)\", ascending=False)\n",
    "\n",
    "print(\"\\n--- Final Model Comparison Summary (Sorted by F1-Score) ---\")\n",
    "print(results_df)"
   ],
   "id": "e33b46d3ee6d8c52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, roc_auc_score, classification_report\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def tune_model_with_gridsearch(\n",
    "        estimator: ClassifierMixin,\n",
    "        param_grid: Dict[str, Any],\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.Series\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"\n",
    "    Performs a GridSearchCV to find the best model.\n",
    "\n",
    "    Optimizes for the F1-score of the positive class ('Certified').\n",
    "    \"\"\"\n",
    "    # We optimize for the F1 score of the positive class (Certified = 1)\n",
    "    f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        scoring=f1_scorer,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"Starting GridSearchCV for {estimator.__class__.__name__}...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"\\nBest parameters found: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "\n",
    "def evaluate_tuned_model(\n",
    "        model: ClassifierMixin,\n",
    "        X_test: pd.DataFrame,\n",
    "        y_test: pd.Series\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Evaluates the tuned model and returns a dictionary of metrics.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"AUC-ROC\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"Report_Str\": classification_report(y_test, y_pred, target_names=['Denied (0)', 'Certified (1)']),\n",
    "        \"F1-Score (Tuned)\": f1_score(y_test, y_pred, pos_label=1)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# Task 4.4: Perform hyperparameter tuning for XGBoost\n",
    "print(\"--- Task 4.4: Hyperparameter Tuning for XGBoost ---\")\n",
    "\n",
    "# Calculate scale_pos_weight for handling imbalance\n",
    "# This is count(negative_class) / count(positive_class)\n",
    "class_counts = y_train.value_counts()\n",
    "scale_pos_weight = class_counts[0] / class_counts[1]\n",
    "print(f\"Calculated 'scale_pos_weight' for imbalance: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],  # Number of trees\n",
    "    'max_depth': [5, 10],  # Max depth of trees\n",
    "    'learning_rate': [0.05, 0.1],  # Step size shrinkage\n",
    "    'scale_pos_weight': [scale_pos_weight]  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# Create the base model\n",
    "xgb_base = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Run the tuning function\n",
    "best_xgb_model = tune_model_with_gridsearch(\n",
    "    estimator=xgb_base,\n",
    "    param_grid=xgb_param_grid,\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train\n",
    ")\n",
    "\n",
    "# --- Task 4.5: Evaluate the best-performing model ---\n",
    "print(\"\\n--- Task 4.5: Evaluating Tuned XGBoost Model ---\")\n",
    "\n",
    "tuned_results = evaluate_tuned_model(best_xgb_model, X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Accuracy: {tuned_results['Accuracy']:.4f}\")\n",
    "print(f\"AUC-ROC: {tuned_results['AUC-ROC']:.4f}\")\n",
    "print(\"\\nClassification Report (Tuned Model):\")\n",
    "print(tuned_results['Report_Str'])\n",
    "\n",
    "# --- Comparison ---\n",
    "print(\"\\n--- Comparison ---\")\n",
    "# Get the baseline F1-score from our previous cell's results\n",
    "baseline_xgb_f1 = results_df.loc['XGBoost', 'F1-Score (Certified)']\n",
    "print(f\"Baseline XGBoost F1-Score (Certified): {baseline_xgb_f1:.4f}\")\n",
    "print(f\"Tuned XGBoost F1-Score (Certified): {tuned_results['F1-Score (Tuned)']:.4f}\")"
   ],
   "id": "18e1de0137384dd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import make_scorer, f1_score\n",
    "#\n",
    "# # Task 4.4: Perform hyperparameter tuning\n",
    "# print(\"--- Task 4.4: Hyperparameter Tuning for Random Forest (Revised: n_jobs=1) ---\")\n",
    "#\n",
    "# # Define the same parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_depth': [10, 20],\n",
    "#     'class_weight': ['balanced', None]\n",
    "# }\n",
    "#\n",
    "# # We will optimize for the F1 score of the positive class (Certified = 1)\n",
    "# f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "#\n",
    "# # Initialize GridSearchCV\n",
    "# # cv=3 means 3-fold cross-validation\n",
    "# # n_jobs=1 runs on a single core, avoiding the pickling error\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=RandomForestClassifier(random_state=42),\n",
    "#     param_grid=param_grid,\n",
    "#     scoring=f1_scorer,\n",
    "#     cv=3,\n",
    "#     n_jobs=1,  # Set to 1 to avoid parallel processing errors\n",
    "#     verbose=1\n",
    "# )\n",
    "#\n",
    "# # Fit the grid search on the TRAINING data\n",
    "# print(\"Starting GridSearchCV... (This may take a few minutes)\")\n",
    "# grid_search.fit(X_train, y_train)\n",
    "#\n",
    "# # Get the best parameters\n",
    "# best_params = grid_search.best_params_\n",
    "# print(f\"\\nBest parameters found: {best_params}\")\n",
    "#\n",
    "# # --- Task 4.5: Evaluate the best-performing model ---\n",
    "# print(\"\\n--- Task 4.5: Evaluating Tuned Model ---\")\n",
    "#\n",
    "# # Get the best model from the grid search\n",
    "# best_rf_model = grid_search.best_estimator_\n",
    "#\n",
    "# # Make predictions on the test set\n",
    "# y_pred_tuned = best_rf_model.predict(X_test)\n",
    "# y_pred_proba_tuned = best_rf_model.predict_proba(X_test)[:, 1]\n",
    "#\n",
    "# # Evaluate performance\n",
    "# accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "# auc_roc_tuned = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "#\n",
    "# print(f\"--- Results for Tuned Random Forest ---\")\n",
    "# print(f\"Accuracy: {accuracy_tuned:.4f}\")\n",
    "# print(f\"AUC-ROC: {auc_roc_tuned:.4f}\")\n",
    "# print(\"\\nClassification Report (Tuned Model):\")\n",
    "# print(classification_report(y_test, y_pred_tuned, target_names=['Denied (0)', 'Certified (1)']))\n",
    "#\n",
    "# # Compare with baseline Random Forest\n",
    "# print(\"\\n--- Comparison ---\")\n",
    "# print(\"Baseline Random Forest F1-Score (Certified): 0.531\")\n",
    "# f1_tuned = f1_score(y_test, y_pred_tuned, pos_label=1)\n",
    "# print(f\"Tuned Random Forest F1-Score (Certified): {f1_tuned:.4f}\")\n",
    "#\n",
    "# # Store the final model and feature names for later\n",
    "# final_model = best_rf_model\n",
    "# feature_names = X_train.columns.tolist()"
   ],
   "id": "45a823be9dcd8bab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_feature_importances(model: ClassifierMixin, feature_names: List[str]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Extracts feature importances from a trained model\n",
    "    and returns them as a sorted Series.\n",
    "    \"\"\"\n",
    "    importances = model.feature_importances_\n",
    "    importance_series = pd.Series(importances, index=feature_names)\n",
    "    return importance_series.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def plot_feature_importances(importance_series: pd.Series):\n",
    "    \"\"\"\n",
    "    Takes a sorted Series of feature importances and plots\n",
    "    them as a horizontal bar chart.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=importance_series.values, y=importance_series.index, palette='rocket')\n",
    "    plt.title('Top Features Influencing Visa Approval (from Tuned XGBoost)')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# Task 5.1 & 5.2: Model Interpretation and Reporting\n",
    "print(\"--- Task 5.1 & 5.2: Interpreting the Best Model ---\")\n",
    "\n",
    "# We'll use the best model from the tuning step in Cell 29\n",
    "final_model = best_xgb_model\n",
    "\n",
    "# We get the feature names from our scaled training data\n",
    "feature_names = X_train_scaled.columns.tolist()\n",
    "\n",
    "# 1. Get the feature importance data\n",
    "feature_importances = get_feature_importances(final_model, feature_names)\n",
    "\n",
    "# 2. Print the results\n",
    "print(\"--- Feature Importances (from Tuned XGBoost) ---\")\n",
    "print(feature_importances)\n",
    "\n",
    "# 3. Plot the results\n",
    "#\n",
    "plot_feature_importances(feature_importances)"
   ],
   "id": "30a5cfabae0dbdd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def get_grouped_feature_importances(importance_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Groups importances from one-hot encoded features by summing them.\n",
    "\n",
    "    Note: This is an underestimate due to `drop_first=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start with the features that are already single\n",
    "    grouped_importances = importance_series.to_dict()\n",
    "\n",
    "    # Define the OHE groups\n",
    "    ohe_groups = {\n",
    "        'education_of_employee': 'education_of_employee_',\n",
    "        'continent': 'continent_',\n",
    "        'region_of_employment': 'region_of_employment_'\n",
    "    }\n",
    "\n",
    "    # Store the new summed importances\n",
    "    new_sums = {}\n",
    "\n",
    "    # Loop through each group\n",
    "    for group_name, prefix in ohe_groups.items():\n",
    "        # Find all features that match the prefix\n",
    "        keys_to_sum = [key for key in grouped_importances.keys() if key.startswith(prefix)]\n",
    "\n",
    "        # Calculate the sum\n",
    "        group_sum = sum(grouped_importances[key] for key in keys_to_sum)\n",
    "\n",
    "        # Remove the individual OHE features\n",
    "        for key in keys_to_sum:\n",
    "            del grouped_importances[key]\n",
    "\n",
    "        # Add the new grouped feature\n",
    "        new_sums[group_name] = group_sum\n",
    "\n",
    "    # Add the new sums to the dictionary\n",
    "    grouped_importances.update(new_sums)\n",
    "\n",
    "    # Convert back to a sorted Series\n",
    "    return pd.Series(grouped_importances).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def plot_grouped_importances(importance_series: pd.Series):\n",
    "    \"\"\"Plots the new grouped feature importances.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=importance_series.values, y=importance_series.index, palette='viridis')\n",
    "    plt.title('Grouped Feature Importances (from Tuned XGBoost)')\n",
    "    plt.xlabel('Importance Score (Summed for OHE Features)')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# We'll use 'feature_importances' from the previous cell (Cell 30)\n",
    "print(\"--- Grouping Feature Importances ---\")\n",
    "\n",
    "grouped_importances = get_grouped_feature_importances(feature_importances)\n",
    "\n",
    "print(grouped_importances)\n",
    "\n",
    "# Plot the new, grouped importances\n",
    "#\n",
    "plot_grouped_importances(grouped_importances)"
   ],
   "id": "666342a3d421340f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import joblib\n",
    "\n",
    "# 'final_model' is the tuned XGB Boost Model\n",
    "# 'fitted_scaler' is the StandardScaler object\n",
    "# 'feature_names' is the list of 18 column names\n",
    "\n",
    "joblib.dump(final_model, 'easy_visa_model.pkl')\n",
    "joblib.dump(fitted_scaler, 'easy_visa_scaler.pkl')\n",
    "joblib.dump(feature_names, 'easy_visa_features.pkl')\n",
    "\n",
    "print(\"Model, scaler, and feature list saved.\")"
   ],
   "id": "e0cee409b018e69b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
